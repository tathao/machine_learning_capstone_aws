{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory Monitoring Using Machine Learning and Computer Vision\n",
    "## Introduction\n",
    "This notebook as a comprehensive guide for developing a machine learning pipeline to address inventory monitoring challenges in distribution centers. The project aims to automate object detection and counting in bins using computer vision techniques, leveraging the Amazon Bin Image Dataset. The approach involves training a model using AWS SageMaker and deploying it for real-time inference.\n",
    "\n",
    "The workflow for this project includes the following key steps:\n",
    "\n",
    "1. **Data Acquisition:** Accessing the Amazon Bin Image Dataset, containing over 500,000 labeled images.\n",
    "2. **Data Preprocessing:** Cleaning, normalizing, and augmenting the dataset to enhance model robustness and generalization.\n",
    "3. **Exploratory Data Analysis (EDA):** Understanding data distribution, variability, and anomalies through visualization.\n",
    "4. **Model Selection and Training:** Using a pre-trained ResNet-50 as a baseline model, fine-tuned on the bin images, with experiments on other architectures.\n",
    "5. **Model Evaluation:** Measuring performance with metrics like accuracy, precision, recall, and F1-score.\n",
    "6. **Deployment:** Deploying the best-performing model as an endpoint in AWS SageMaker for real-time predictions.\n",
    "7. **Monitoring:** Continuously tracking the model’s performance to ensure reliability over time.\n",
    "\n",
    "The proposed pipeline demonstrates an efficient and scalable solution for automating inventory monitoring, ensuring accuracy, and optimizing operations in supply chain distribution centers.\n",
    "\n",
    "**Note:** This notebook has a bunch of code and markdown cells with TODOs that you have to complete. These are meant to be helpful guidelines for you to finish your project while meeting the requirements in the project rubrics. Feel free to change the order of the TODO's and/or use more than one cell to complete all the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install smdebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install any packages that you might need\n",
    "\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from src.data_splitter import DataSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import any packages that you might need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "**TODO:** Run the cell below to download the data.\n",
    "\n",
    "The cell below creates a folder called `train_data`, downloads training data and arranges it in subfolders. Each of these subfolders contain images where the number of objects is equal to the name of the folder. For instance, all images in folder `1` has images with 1 object in them. Images are not divided into training, testing or validation sets. If you feel like the number of samples are not enough, you can always download more data (instructions for that can be found [here](https://registry.opendata.aws/amazon-bin-imagery/)). However, we are not acessing you on the accuracy of your final trained model, but how you create your machine learning engineering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_arrange_data():\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    with open('file_list.json', 'r') as f:\n",
    "        d=json.load(f)\n",
    "\n",
    "    for k, v in d.items():\n",
    "        print(f\"Downloading Images with {k} objects\")\n",
    "        directory=os.path.join('train_data', k)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        for file_path in tqdm(v):\n",
    "            file_name=os.path.basename(file_path).split('.')[0]+'.jpg'\n",
    "            s3_client.download_file('aft-vbi-pds', os.path.join('bin-images', file_name),\n",
    "                             os.path.join(directory, file_name))\n",
    "\n",
    "download_and_arrange_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The **Amazon Bin Image Dataset** is a specialized dataset designed for machine learning applications in inventory monitoring and object detection. It comprises images captured from distribution center bins, each annotated with metadata about the number and type of objects present.\n",
    "\n",
    "1. The dataset contains **536,434** images, includes bins with varying numbers of objects (e.g., 1, 2, 3, 4, 5 objects per image)\n",
    "2. The dataset is organized into five classes, based on the number of objects in the bin:\n",
    "\n",
    "**Class 1:** Images with one object in the bin.\n",
    "**Class 2:** Images with two objects in the bin.\n",
    "**Class 3:** Images with three objects in the bin.\n",
    "**Class 4:** Images with four objects in the bin.\n",
    "**Class 5:** Images with five objects in the bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"train_data\"\n",
    "output_dir = \"data\"\n",
    "splitter = DataSplitter(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage the data sets to train, test and validation sets.\n",
    "splitter.execute_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell below is upload the output_dir to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_directory_to_s3(local_directory, s3_bucket, s3_prefix=''):\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for file in tqdm(files):\n",
    "            local_path = os.path.join(root, file)\n",
    "            s3_path = os.path.join(s3_prefix, local_path).replace(\"\\\\\", \"/\")\n",
    "            s3_client.upload_file(local_path, s3_bucket, s3_path)\n",
    "    print('upload complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "bucket_name = 'haont1-bucket'\n",
    "s3_ds_directory = 'data'\n",
    "role = sagemaker.get_execution_role()\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_directory_to_s3(output_dir, bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_set_eda import DatasetEDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the distribution of object in bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"data/train\"\n",
    "eda = DatasetEDA(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of object in bins in training set\n",
    "eda.analyze_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Class **3** has the highest number of images (1866) while class **1** has the lowest (859). This one shows a potential class imbalance where some classes have significantly fewer examples compared to others. Class **3** has approximately **2.17 times** more than class **1**, which may lead to biases during training.\n",
    "* Classes **2** and **4** have a relatively balanced number of images(1609 and 1661), closer to the overall average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.visualize_samples(num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From samples image we could see:\n",
    "\n",
    "1. Variability:\n",
    "\n",
    "   * The dataset contains a diverse set of items stored in bins.\n",
    "   * Items are different about size, shape, texture and packaking materials.\n",
    "   * Some items are boxed, others are wrapped in plastic, while some appear loosely packed.\n",
    "3. Complexity:\n",
    "\n",
    "   * The images include multiple types of objects, that making classification become challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies\n",
    "anomalies = eda.detect_anomalies()\n",
    "if anomalies:\n",
    "    print(f\"Total anomalies found: {len(anomalies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding **7307 anomalies** in your training set indicates that there are instances or data points that deviate significantly from the majority of the dataset's patterns or expected behavior\n",
    "\n",
    "* **Potential errors in data labeling:** Some images may be mislabeled, leading to incorrect class assignments. For example, an image belonging to class **3** might be incorrectly labeled as class **1**.\n",
    "* **Corrupt or noisy data:** The anomalies could result from poor image quality, occlusions, or distortions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "* Resize image to standardize input dimensions\n",
    "* Normalize pixel values to enhance model convergence\n",
    "* Data augmentation techniques (resizing, flipping, rotation and normalization) to increase dataset diversity and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directories containing the train, validation, and test splits\n",
    "train_dir = \"data/train\"\n",
    "val_dir = \"data/val\"\n",
    "test_dir = \"data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import DataLoaderCreator\n",
    "\n",
    "# Initialize DataLoaderCreator\n",
    "loader_creator = DataLoaderCreator(train_dir, val_dir, test_dir)\n",
    "\n",
    "# Create DataLoaders\n",
    "data_loaders = loader_creator.create_data_loaders()\n",
    "\n",
    "# Example: Accessing the DataLoader for training\n",
    "train_loader = data_loaders[\"train\"]\n",
    "print(f\"Number of batches in training DataLoader: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize image\n",
    "\n",
    "* Resizes all images to a standard dimension (224x224 in this case) to standardize the input size for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show resize image to standardize input dimensions\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def display_resized_images(data_loader, num_images=5):\n",
    "    \"\"\"\n",
    "    Display a few resized images from the DataLoader.\n",
    "    \"\"\"\n",
    "    # Get a batch of data\n",
    "    images, labels = next(iter(data_loader))\n",
    "\n",
    "    # Create a grid for displaying images\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(min(num_images, len(images))):\n",
    "        # Denormalize the image for visualization\n",
    "        img = images[i]  # Shape: [C, H, W]\n",
    "        img = F.to_pil_image(img)  # Convert tensor to PIL Image\n",
    "\n",
    "        # Display the image\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Label: {labels[i]}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Display the resized images\n",
    "display_resized_images(train_loader, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Images\n",
    "\n",
    "* Normalizes pixel values using ImageNet's mean and standard deviation to standardize the input pixel intensity range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def display_normalized_images(data_loader, num_images=5):\n",
    "    \"\"\"\n",
    "    Display a few normalized images from the DataLoader.\n",
    "    \"\"\"\n",
    "    # Get a batch of data\n",
    "    images, labels = next(iter(data_loader))\n",
    "    \n",
    "    \n",
    "    # Create a grid for displaying images\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(min(num_images, len(images))):\n",
    "        # Original normalized image\n",
    "        normalized_img = images[i]  # Shape: [C, H, W]\n",
    "             \n",
    "        # Display normalized images\n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(F.to_pil_image(normalized_img))  # Normalized image\n",
    "        plt.title(f\"Normalized\\nLabel: {labels[i]}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Display the normalized and denormalized images\n",
    "display_normalized_images(train_loader, num_images=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "1. **Random Hozirontal Flip:** Flips images horizontally with a 50% probability, providing variety in image orientation\n",
    "2. **Random Rotation:** Rotates images randomly within a ±15° range, introducing positional diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def display_augmentations(data_loader, num_images=5, num_augmentations=3):\n",
    "    \"\"\"\n",
    "    Display augmented images to demonstrate the effect of data augmentation techniques.\n",
    "    \"\"\"\n",
    "    # Get a batch of data\n",
    "    images, labels = next(iter(data_loader))\n",
    "\n",
    "    # Create a grid for displaying images\n",
    "    plt.figure(figsize=(15, num_augmentations * 5))\n",
    "    for i in range(min(num_images, len(images))):\n",
    "        for j in range(num_augmentations):\n",
    "            # Apply augmentation multiple times\n",
    "            augmented_img = images[i]\n",
    "            augmented_img = F.to_pil_image(augmented_img)  # Convert tensor to PIL Image for display\n",
    "            \n",
    "            # Display the image\n",
    "            plt.subplot(num_images, num_augmentations, i * num_augmentations + j + 1)\n",
    "            plt.imshow(augmented_img)\n",
    "            if j == 0:\n",
    "                plt.title(f\"Original\\nLabel: {labels[i]}\")\n",
    "            else:\n",
    "                plt.title(f\"Augmented #{j}\\nLabel: {labels[i]}\")\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Display augmented images\n",
    "display_augmentations(train_loader, num_images=5, num_augmentations=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "**TODO:** This is the part where you can train a model. The type or architecture of the model you use is not important. \n",
    "\n",
    "**Note:** You will need to use the `train.py` script to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SM_CHANNEL_TRAINING']=f\"s3://{bucket_name}/{s3_ds_directory}/train/\"\n",
    "os.environ['SM_CHANNEL_VALIDATION']=f\"s3://{bucket_name}/{s3_ds_directory}/val/\"\n",
    "os.environ['SM_CHANNEL_TEST']=f\"s3://{bucket_name}/{s3_ds_directory}/test/\"\n",
    "os.environ[\"SM_MODEL_DIR\"] = f\"s3://{bucket_name}/model/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Declare your model training hyperparameter.\n",
    "#NOTE: You do not need to do hyperparameter tuning. You can use fixed hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create your training estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit your estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standout Suggestions\n",
    "You do not need to perform the tasks below to finish your project. However, you can attempt these tasks to turn your project into a more advanced portfolio piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "**TODO:** Here you can perform hyperparameter tuning to increase the performance of your model. You are encouraged to \n",
    "- tune as many hyperparameters as you can to get the best performance from your model\n",
    "- explain why you chose to tune those particular hyperparameters and the ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "#Declare your HP ranges, metrics etc.\n",
    "hyperparameter_ranges = {\n",
    "    'batch_size': IntegerParameter(32, 128),\n",
    "    'learning_rate': ContinuousParameter(1e-4, 5e-2),\n",
    "    'num_epochs': IntegerParameter(5, 15)\n",
    "}\n",
    "\n",
    "objective_metric_name = 'Average loss'\n",
    "objective_type = 'Minimize'\n",
    "\n",
    "metric_definitions = [{\"Name\": \"Average loss\", \"Regex\": \"Average loss: ([0-9\\\\.]+)\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Create your training estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"hpo.py\",\n",
    "    source_dir=\"./src\",\n",
    "    role=role,\n",
    "    framework_version='1.12',\n",
    "    py_version='py38',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge', # Use GPU-enabled instance\n",
    ")\n",
    "\n",
    "# Define hyperparameter tuner\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_metric_name=objective_metric_name,\n",
    "    objective_type=objective_type,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_jobs=5,  # Number of total jobs\n",
    "    max_parallel_jobs=2  # Number of jobs to run in parallel\n",
    ")\n",
    "\n",
    "print(\"Estimator and tuner defined with S3 model directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your estimator\n",
    "tuner.fit({\n",
    "    \"train\": os.environ['SM_CHANNEL_TRAINING'],\n",
    "    \"validation\": os.environ['SM_CHANNEL_VALIDATION'],\n",
    "    \"test\": os.environ['SM_CHANNEL_TEST']\n",
    "}, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the best hyperparameters\n",
    "best_estimator = tuner.best_estimator()\n",
    "\n",
    "#Get the hyperparameters of the best trained model\n",
    "best_hyperparameters = best_estimator.hyperparameters()\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_batch_size = int(best_hyperparameters[\"batch_size\"])\n",
    "best_learning_rate = float(best_hyperparameters[\"learning_rate\"])\n",
    "best_epochs = int(best_hyperparameters[\"num_epochs\"])\n",
    "\n",
    "print(\"Best batch size: \", best_batch_size)\n",
    "print(\"Best learning rate: \", best_learning_rate)\n",
    "print(\"Best epochs: \", best_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the tuning results\n",
    "\n",
    "The best model used a batch size of 108 and a learning rate of 0.001369 and testing score is 23.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp = HyperparameterTuningJobAnalytics(\n",
    "  hyperparameter_tuning_job_name='pytorch-training-241202-0346')\n",
    "\n",
    "jobs = exp.dataframe()\n",
    "\n",
    "jobs.sort_values('FinalObjectiveValue', ascending=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare to perform Training on Best Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator=tuner.best_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If kernel die, contienue from a completed training job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BetterTrainingJobName='pytorch-training-241202-0346-005-5fef11df'\n",
    "my_estimator = sagemaker.estimator.Estimator.attach(BetterTrainingJobName)\n",
    "my_estimator.hyperparameters()\n",
    "best_estimator=my_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"batch_size\": int(best_estimator.hyperparameters()['batch_size'].replace('\"', '')), \\\n",
    "                   \"learning_rate\": best_estimator.hyperparameters()['learning_rate'], \\\n",
    "                   \"num_epochs\": int(best_estimator.hyperparameters()['num_epochs'].replace('\"', ''))\n",
    "                  }\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Profiling and Debugging\n",
    "**TODO:** Use model debugging and profiling to better monitor and debug your model training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import (\n",
    "    Rule,\n",
    "    ProfilerRule,\n",
    "    rule_configs,\n",
    "    DebuggerHookConfig,\n",
    "    ProfilerConfig,\n",
    "    FrameworkProfile,\n",
    "    CollectionConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up debugging and profiling rules and hooks\n",
    "\n",
    "rules = [\n",
    "    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    Rule.sagemaker(rule_configs.overtraining()),\n",
    "    Rule.sagemaker(rule_configs.poor_weight_initialization()),\n",
    "    ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "]\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500, framework_profile_params=FrameworkProfile(num_steps=10)\n",
    ")\n",
    "\n",
    "collection_config_list = [\n",
    "    CollectionConfig(\n",
    "        name=\"CrossEntropyLoss_output_0\",\n",
    "        parameters={\n",
    "            \"include_regex\": \"CrossEntropyLoss_output_0\", \n",
    "            \"train.save_interval\": \"50\",\n",
    "            \"eval.save_interval\": \"1\"\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "debugger_hook_config = DebuggerHookConfig(\n",
    "    hook_parameters={\"train.save_interval\": \"500\", \"eval.save_interval\": \"50\"},\n",
    "    collection_configs=collection_config_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"./src\",\n",
    "    role=role,\n",
    "    framework_version='1.12',\n",
    "    py_version='py38',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge', # Use GPU-enabled instance          \n",
    "    hyperparameters=hyperparameters,\n",
    "    debugger_hook_config=debugger_hook_config,\n",
    "    profiler_config=profiler_config,\n",
    "    rules=rules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the estimator\n",
    "estimator.fit({\"train\": os.environ['SM_CHANNEL_TRAINING'], \"test\": os.environ['SM_CHANNEL_TEST']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a debugging output.\n",
    "from smdebug.trials import create_trial\n",
    "from smdebug.core.modes import ModeKeys\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "\n",
    "trial = create_trial(estimator.latest_job_debugger_artifacts_path())\n",
    "\n",
    "def get_data(trial, tname, mode):\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps(mode=mode)\n",
    "    vals = []\n",
    "    for s in steps:\n",
    "        vals.append(tensor.value(s, mode=mode))\n",
    "    return steps, vals\n",
    "\n",
    "\n",
    "def plot_tensor(trial, tensor_name):\n",
    "\n",
    "    steps_train, vals_train = get_data(trial, tensor_name, mode=ModeKeys.TRAIN)\n",
    "    print(\"loaded TRAIN data\")\n",
    "    steps_eval, vals_eval = get_data(trial, tensor_name, mode=ModeKeys.EVAL)\n",
    "    print(\"loaded EVAL data\")\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    host = host_subplot(111)\n",
    "\n",
    "    par = host.twiny()\n",
    "\n",
    "    host.set_xlabel(\"Steps (TRAIN)\")\n",
    "    par.set_xlabel(\"Steps (EVAL)\")\n",
    "    host.set_ylabel(tensor_name)\n",
    "\n",
    "    (p1,) = host.plot(steps_train, vals_train, label=tensor_name)\n",
    "    print(\"completed TRAIN plot\")\n",
    "    (p2,) = par.plot(steps_eval, vals_eval, label=\"val_\" + tensor_name)\n",
    "    print(\"completed EVAL plot\")\n",
    "    leg = plt.legend()\n",
    "\n",
    "    host.xaxis.get_label().set_color(p1.get_color())\n",
    "    leg.texts[0].set_color(p1.get_color())\n",
    "\n",
    "    par.xaxis.get_label().set_color(p2.get_color())\n",
    "    leg.texts[1].set_color(p2.get_color())\n",
    "\n",
    "    plt.ylabel(tensor_name)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_tensor(trial, \"CrossEntropyLoss_output_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Is there some anomalous behaviour in your debugging output? If so, what is the error and how will you fix it?  \n",
    "**TODO**: If not, suppose there was an error. What would that error look like and how would you have fixed it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display the profiler output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deploying and Querying\n",
    "**TODO:** Can you deploy your model to an endpoint and then query that endpoint to get a result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Deploy your model to an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run an prediction on the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remember to shutdown/delete your endpoint once your work is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheaper Training and Cost Analysis\n",
    "**TODO:** Can you perform a cost analysis of your system and then use spot instances to lessen your model training cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train your model using a spot instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Instance Training\n",
    "**TODO:** Can you train your model on multiple instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train your model on Multiple Instances"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
